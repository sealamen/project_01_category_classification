# -*- coding: utf-8 -*-
"""prj1_news_category_classification01_1_crawling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14i5Aai62FoYSseFqrVafTOn2WxL4tjWa
"""

from bs4 import BeautifulSoup
import requests
import re
import pandas as pd
import datetime

print(datetime.datetime.today().strftime('%Y%m%d'))

category = ['Politics', 'Economic', 'Social', 'Culture', 'World', 'IT']

# 항목은 다음 6개

# 일단은 주소가 필요해요. 웹페이지에서 가져올거에요(네이버뉴스)
url = 'https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=100'

headers = {"User-Agent":'Mozilla/5.0 (Windows NT 10.0: Win64: x64) AppleWebKit/537.36(KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36'}
resp = requests.get(url, headers = headers)

# url로 요청을 하고 응답을 받은게 resp
# 근데 requests 명령이 차단될 수 있어(네이버에서 로봇이 크로울링하는걸 막아서)
# 그럴 때 header를 쓰면 브라우저가 요청하는 것 처럼 요청을 보낼 수 있어
# chrome, safari 다 주자. 하나만 써도 되긴 해

print(list(resp))
print(type(resp))
# 이렇게 사이트에서 우클릭 > 검사 했을 때 나오는게 나와
# 타입은 request객체. 근데 리스트로 들어있으니 위에처럼 list(resp)하면 볼 수 있어

soup = BeautifulSoup(resp.text, 'html.parser')
print(soup)
# html.parser를 통해 soup을 하면 html문서와 같은 형태로 되어


title_tags = soup.select('.cluster_text_headline')
print(title_tags)
exit()
# 그중에서 class가 cluster_test_headline에 묶여있는걸 싸그리 가져와버리는게 soup.select
# 클래스명 앞에 .을 붙여야해
# 크로울링 할때는 일일이 홈페이지가서 저렇게 긁어와서 코드를 만들어야해.

# 여기서 이제 타이틀만 뽑아내볼게요

titles = []
for title_tag in title_tags:
  titles.append(re.compile('[^가-힣|a-z|A-Z ]').sub(' ', title_tag.text))
print(titles)
print(len(titles))

# 모든 한글(처음부터 마지막까지), 모든 소문자, 모든 대문자, 공백문자를 제외한(^) 
# 모든 것을 빼겠다(sub) (정확하게는 ' '띄어쓰기로 받겠다.)
# 여기에 숫자를 추가하고 싶으면 0-9 넣어주면 되고, or연산자(|) 없어도 되긴 함.
# 마지막에 .text를 추가하는 것은 tag객체이기 때문에 tag안에 들어있는 text를 꺼내오는 것
# 총 44개의 제목 리스트가 들어오게돼(정치섹션의 1페이지)

pd.set_option("display.unicode.east_asian_width", True)

df_titles = pd.DataFrame()   # 빈 데이터프레임을 하나 만들어놓고,
re_title = re.compile('[^가-힣|a-z|A-Z ]')   # re객체 하나 만들어놓자
titles = []
for i in range(6):  
  resp = requests.get('https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=10{}'.format(i), headers = headers)
  soup = BeautifulSoup(resp.text, 'html.parser')
  title_tags = soup.select(' .cluster_text_headline')
  for title_tag in title_tags:
    titles.append(re_title.sub(' ', title_tag.text))
  df_section_titles = pd.DataFrame(titles, columns=['title'])
  df_section_titles['category'] = category[i]
  df_titles = pd.concat([df_titles, df_section_titles], axis='rows', ignore_index=True)
  print('https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=10%d'%(i))
  # 어디까지 진행되나 확인

print(len(titles))
print(df_titles.head())
print(df_titles.info())
print(df_titles.value_counts('category'))

# 카테고리 6개이니 레인2ㅣ 6
# 주소 봤더니 100에서 101,102,103 이런식으로 숫자만 바뀌어. 그자리만 i로 바꿔줘(문자열포매터)
# 숫자열 포매터(%d는 보통 print에서만 먹히는데 여기서도 되긴해. 그래도 문자열쓰자
# 그 타이틀들을 데이터프레임으로 받아주자
# concat을 줘서 빈 데이터프레임에다가 df sectiontitle 만들고, ...

# i가 0일때거를 쭉 뽑아올거야. 그걸 크로울링해서 갯수만큼 쭉 가져올거야.ex)0~23(24개)). 
# 그리고 카테고리를 만들어서 '정치'라고 된 애들을 24개만큼 쭉 만들어
# 그리고 얘네를 합쳐(concat)

# 헤드라인 뉴스를 이렇게 쭉 가져오는 작업!!

print(titles[0])
print(titles[-1])

print(df_titles.head())

df_titles.to_csv('./crawling/naver.headline_news{}.csv'.format(
  datetime.datetime.today().strftime('%Y%m%d')), index=False)

